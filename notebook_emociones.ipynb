{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 🚀 1. Importar librerías necesarias\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Bidirectional # Añadido Bidirectional\n# from tensorflow.keras import regularizers # No se usa L2 ahora\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n# from sklearn.utils.class_weight import compute_class_weight # No se usa\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport re\nimport unicodedata\nimport matplotlib.pyplot as plt\nimport os\nimport pickle\nimport gzip\nimport subprocess","metadata":{"_uuid":"1f44cdc8-06e8-456f-be28-b12bd21f7fdd","_cell_guid":"9056de0f-4879-4ff2-b018-70c382c4db5a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 1b. Descargar los embeddings fastText en español (ACTIVADO)\nembed_path = '/kaggle/working/cc.es.300.vec.gz'\nembed_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz'\nPRETRAINED_EMBEDDING_DIM = 300\n\nif not os.path.exists(embed_path):\n    print(f\"Descargando embeddings fastText desde {embed_url}...\")\n    try:\n        result = subprocess.run(['wget', '-O', embed_path, embed_url], check=True, capture_output=True, text=True)\n        print(\"Descarga completada.\")\n    except Exception as e:\n        print(f\"Error al descargar embeddings: {e}\")\n        embed_path = None # Fallback\nelse:\n   print(f\"Embeddings fastText ya existen en {embed_path}.\")\nif not os.path.exists(embed_path): embed_path = None # Doble check\nif embed_path: print(\"Embeddings pre-entrenados ACTIVADOS.\")\nelse: print(\"Embeddings pre-entrenados DESACTIVADOS.\"); exit() # Salir si no hay embeddings","metadata":{"_uuid":"73e80fe1-4c1c-4885-b58c-59819582ab2f","_cell_guid":"b98ea627-3d34-4ce4-923e-865444de8aaf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 1c. Cargar embeddings fastText (ACTIVADO)\nembeddings_index = {}\nnum_loaded = 0\nif embed_path and os.path.exists(embed_path):\n    print(f\"Cargando embeddings desde {embed_path}...\")\n    try:\n        with gzip.open(embed_path, 'rt', encoding='utf-8', newline='\\n', errors='ignore') as f:\n            header = f.readline().split(); print(f\"Cabecera: {header}\")\n            for line in f:\n                values = line.rstrip().split(' ')\n                word = values[0]\n                if len(values) == PRETRAINED_EMBEDDING_DIM + 1:\n                   try:\n                       coefs = np.asarray(values[1:], dtype='float32')\n                       embeddings_index[word] = coefs; num_loaded += 1\n                   except ValueError: pass\n    except Exception as e: print(f\"Error al leer embeddings: {e}\"); embeddings_index = {}\n    print(f'Se cargaron {num_loaded} vectores.')\n    if not embeddings_index: print(\"ADVERTENCIA: Falló carga de embeddings.\"); exit() # Salir si falló\nelse: print(\"Archivo de embeddings no encontrado.\"); exit() # Salir si no hay archivo","metadata":{"_uuid":"97c8659f-8e75-491e-bf41-9b6ce3b1dd63","_cell_guid":"659e8779-21db-4702-b686-9b19fbd94924","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 2. Configurar uso de GPU si está disponible\n# (Código igual)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e: print(e)\nelse: print(\"No se detectó GPU, se usará CPU.\")","metadata":{"_uuid":"839cc08a-240a-4f99-b182-0a1505713842","_cell_guid":"0ba78652-cdc2-440e-a8d1-57dc14ae3844","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 3. Cargar tus datos\n# (Código igual)\ntry:\n  df = pd.read_csv('https://raw.githubusercontent.com/adiacla/sentimientos/refs/heads/main/emociones.csv', sep=\"|\")\n  print(\"Datos cargados:\"); print(df.head()); print(f\"\\nForma inicial: {df.shape}\")\nexcept Exception as e: print(f\"Error al cargar CSV: {e}\"); exit()","metadata":{"_uuid":"557b7680-bdef-4098-8e1b-66fd79d48c86","_cell_guid":"5869f497-1a2d-490b-a40f-9df49db7d8b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 4. Preprocesamiento y Limpieza de Texto\n# (Código igual)\nemotion_translation = {\n 'depressed': 'deprimido', 'hopeless': 'desesperanzado', 'lonely': 'solitario', 'suicidal': 'suicida',\n 'disappointment': 'decepción', 'disgust': 'asco', 'remorse': 'remordimiento', 'grief': 'duelo',\n 'embarrassment': 'vergüenza', 'fear': 'miedo', 'nervousness': 'nerviosismo', 'sadness': 'tristeza',\n 'anger': 'enojo', 'neutral': 'neutral'\n}\ndf['emotion_es'] = df['emotion'].map(emotion_translation)\nprint(\"\\nEmociones traducidas:\"); print(df['emotion_es'].value_counts())\ndef clean_text(text):\n  if not isinstance(text, str): return \"\"\n  text = text.lower()\n  text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode('utf-8')\n  text = re.sub(r\"http\\S+\", \"\", text); text = re.sub(r\"@\\w+\", \"\", text)\n  text = re.sub(r\"#\\w+\", \"\", text); text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n  text = re.sub(r'\\s+', ' ', text).strip(); return text\ndf['tweet_clean'] = df['tweet'].apply(clean_text)\nprint(\"\\nTweets limpios (head):\"); print(df[['tweet_clean']].head())\ndf = df.dropna(subset=['tweet_clean', 'emotion_es'])\ndf = df[df['tweet_clean'].str.strip() != '']; print(f\"\\nForma tras limpieza: {df.shape}\")","metadata":{"_uuid":"5e7da848-8a37-49fe-8f49-6692de589e6e","_cell_guid":"960519ea-3efd-4f46-a4b5-8c752fa209a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 5. Tokenizar los textos\n# (Código igual)\noov_token = '<OOV>'; tokenizer = Tokenizer(oov_token=oov_token)\ntokenizer.fit_on_texts(df['tweet_clean'].values); word_index = tokenizer.word_index\nvocab_size = len(word_index) + 1; print(f\"\\nTamaño del vocabulario: {vocab_size}\")\nsequences = tokenizer.texts_to_sequences(df['tweet_clean'].values)","metadata":{"_uuid":"2f72223f-e5a3-4cb4-bc2d-d99e516d22db","_cell_guid":"88ca91fa-5427-4cac-8885-62daa8f91841","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 6. Padding de secuencias\n# (Código igual)\nmaxlen = 50; X = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\nprint(f\"\\nForma de X tras padding: {X.shape}\")","metadata":{"_uuid":"3ea6a7b2-2274-4f03-adec-a3eafda49652","_cell_guid":"528912c4-4921-4324-b147-be6036ce8b45","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 7. Codificar las etiquetas\n# (Código igual)\nlabel_encoder = LabelEncoder(); integer_encoded = label_encoder.fit_transform(df['emotion_es'].values)\nnum_classes = len(label_encoder.classes_); print(f'\\nNum clases: {num_classes}'); print(f'Clases: {label_encoder.classes_}')\ny = to_categorical(integer_encoded, num_classes=num_classes); print(f\"Forma de y (one-hot): {y.shape}\")","metadata":{"_uuid":"033985bd-8ea6-428b-8ad5-f1351a08e565","_cell_guid":"047a5b1c-11a7-4259-83ab-6e19b0319f32","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 8. Dividir datos en entrenamiento y validación\n# (Código igual)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"\\nForma train: {X_train.shape}, {y_train.shape}\"); print(f\"Forma val: {X_val.shape}, {y_val.shape}\")","metadata":{"_uuid":"6acb9aea-9e45-4a31-8d91-7cbb60254bc6","_cell_guid":"7ec18306-36c0-4568-80b2-0f2100149ba7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 10. Crear Matriz de Embedding (ACTIVADO)\n# (Código igual, asegura que use_pretrained=True si embeddings_index tiene datos)\nembedding_matrix = None; use_pretrained = False\nprint(\"Creando matriz de embedding...\")\nembedding_matrix = np.zeros((vocab_size, PRETRAINED_EMBEDDING_DIM)); hits = 0; misses = 0\nif embeddings_index:\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector; hits += 1\n        else: misses += 1\n    print(f\"Palabras encontradas: {hits}\"); print(f\"Palabras no encontradas: {misses}\")\n    if hits > 0: use_pretrained = True; print(\"Se usarán embeddings pre-entrenados.\")\n    else: print(\"ADVERTENCIA: Ninguna palabra encontrada.\"); embedding_matrix = None; use_pretrained = False\nelse: print(\"No se usarán embeddings pre-entrenados.\"); embedding_matrix = None\nembedding_dim_model = PRETRAINED_EMBEDDING_DIM if use_pretrained else 128\nprint(f\"Dimensión de embedding a usar: {embedding_dim_model}\")\nif not use_pretrained: print(\"ADVERTENCIA: Fallback a embeddings entrenables.\"); embedding_dim_model = 128 # Asegurar fallback","metadata":{"_uuid":"0dc02f77-95c7-4f47-ba05-e78aacd9a1c0","_cell_guid":"83b1157e-ffd5-40c4-8062-38666f611de7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:46:45.184491Z","iopub.execute_input":"2025-04-29T16:46:45.185219Z","iopub.status.idle":"2025-04-29T16:46:45.397947Z","shell.execute_reply.started":"2025-04-29T16:46:45.185195Z","shell.execute_reply":"2025-04-29T16:46:45.397137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 11. Construir el modelo LSTM (PARA FASE 1 - Embeddings Congelados)\n\nhidden_units = 64; dense_units = 32; dropout_rate = 0.4\nprint(f\"\\nConstruyendo modelo LSTM para Fase 1\")\n\nmodel = Sequential(name=\"LSTM_Emotion_Classifier_Emb_Fase1\")\nmodel.add(Input(shape=(maxlen,), name=\"Input\"))\n\n# --- Capa Embedding CONGELADA ---\nif use_pretrained and embedding_matrix is not None:\n    print(\"Usando capa Embedding con pesos pre-entrenados (CONGELADOS).\")\n    model.add(Embedding(\n        input_dim=vocab_size, output_dim=embedding_dim_model,\n        weights=[embedding_matrix], \n        trainable=False, # <--- CONGELADO para Fase 1\n        name=\"Pretrained_Embedding\"\n    ))\nelse: # Fallback por si acaso\n    print(f\"ERROR: No se pudieron usar embeddings pre-entrenados. Usando entrenables.\"); embedding_dim_model=128\n    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim_model, input_length=maxlen, trainable=True, name=\"Trainable_Embedding\"))\n\n# --- Resto del Modelo ---\n# Probar con Bidireccional aquí si se desea: model.add(Bidirectional(LSTM(...)))\nmodel.add(LSTM(units=hidden_units, name=\"LSTM_Layer\"))\nmodel.add(Dropout(dropout_rate, name=\"Dropout_LSTM\"))\nmodel.add(Dense(units=dense_units, activation='relu', name=\"Dense_Layer\"))\nmodel.add(Dense(units=num_classes, activation='softmax', name=\"Output_Softmax\"))","metadata":{"_uuid":"96b20f3e-f8fc-4701-91bb-07d84fef1e19","_cell_guid":"e098f258-30f5-4f0c-8fe4-4a8638f66e97","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:47:06.678211Z","iopub.execute_input":"2025-04-29T16:47:06.678477Z","iopub.status.idle":"2025-04-29T16:47:06.883785Z","shell.execute_reply.started":"2025-04-29T16:47:06.678458Z","shell.execute_reply":"2025-04-29T16:47:06.883044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 12. Compilar el modelo (FASE 1 - LR Moderada)\nlearning_rate_fase1 = 0.001 # Tasa de aprendizaje para Fase 1\noptimizer_fase1 = Adam(learning_rate=learning_rate_fase1)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer_fase1, metrics=['accuracy'])\nprint(f\"\\nModelo compilado para Fase 1 con LR={learning_rate_fase1}\")\nmodel.summary()","metadata":{"_uuid":"67772192-c8d6-4e9f-b2c9-83abb794845b","_cell_guid":"ebaed698-5bf2-4ae6-bb99-f042bb4c98b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:47:08.863469Z","iopub.execute_input":"2025-04-29T16:47:08.863747Z","iopub.status.idle":"2025-04-29T16:47:08.884348Z","shell.execute_reply.started":"2025-04-29T16:47:08.863727Z","shell.execute_reply":"2025-04-29T16:47:08.883612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 13. Callbacks (FASE 1)\ncheckpoint_path_fase1 = 'best_model_emociones_emb_fase1.keras'\nmodel_checkpoint_fase1 = ModelCheckpoint(checkpoint_path_fase1, monitor='val_loss', save_best_only=True, verbose=1)\n# Usaremos los mismos ES y ReduceLR, pero podrían definirse distintos si se quisiera\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=False) # False aquí, cargaremos el mejor explícitamente\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6, verbose=1) # Aumentar min_lr un poco\ncallbacks_list_fase1 = [model_checkpoint_fase1, early_stopping, reduce_lr]\nprint(f\"Callbacks definidos para Fase 1. Checkpoint: {checkpoint_path_fase1}\")","metadata":{"_uuid":"a9ee71df-214f-4a11-ac8c-e695c37cbb30","_cell_guid":"cb75b5a1-2e6b-43ed-80ed-56077522c3fd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:47:15.088249Z","iopub.execute_input":"2025-04-29T16:47:15.088945Z","iopub.status.idle":"2025-04-29T16:47:15.094321Z","shell.execute_reply.started":"2025-04-29T16:47:15.088924Z","shell.execute_reply":"2025-04-29T16:47:15.093341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 14. Entrenar el modelo (FASE 1)\nepochs_fase1 = 50 # EarlyStopping definirá el número real\nbatch_size = 128\nprint(\"\\n--- Iniciando Entrenamiento FASE 1 (Embeddings Congelados) ---\")\nhistory_fase1 = model.fit(\n  X_train, y_train,\n  epochs=epochs_fase1, batch_size=batch_size, validation_data=(X_val, y_val),\n  callbacks=callbacks_list_fase1, verbose=1\n)\nprint(\"--- Entrenamiento FASE 1 Finalizado ---\")\nlast_epoch_fase1 = early_stopping.stopped_epoch if early_stopping.stopped_epoch > 0 else epochs_fase1 -1 # Guardar última época real","metadata":{"_uuid":"1924a9cc-d70c-4f2c-9f63-86f13d537bb1","_cell_guid":"fdba3ffa-f94d-468b-86b6-ae7cd65e79bc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:47:21.065720Z","iopub.execute_input":"2025-04-29T16:47:21.065958Z","iopub.status.idle":"2025-04-29T16:48:17.122574Z","shell.execute_reply.started":"2025-04-29T16:47:21.065942Z","shell.execute_reply":"2025-04-29T16:48:17.121952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 14b. Preparar para FASE 2 (Fine-tuning)\nprint(\"\\n--- Preparando FASE 2 (Fine-tuning) ---\")\nmodel_loaded_fase2 = False\ntry:\n    # Cargar el mejor modelo de la Fase 1\n    if os.path.exists(checkpoint_path_fase1):\n        model = load_model(checkpoint_path_fase1) # Cargar explícitamente el mejor\n        print(f\"Mejor modelo de Fase 1 cargado desde {checkpoint_path_fase1}.\")\n\n        # Buscar la capa Embedding y hacerla entrenable\n        embedding_layer = model.get_layer(name='Pretrained_Embedding')\n        if embedding_layer:\n            embedding_layer.trainable = True # <--- DESCONGELAR\n            print(f\"Capa Embedding '{embedding_layer.name}' DESCONGELADA (trainable=True).\")\n        else:\n            print(\"ADVERTENCIA: No se encontró la capa Embedding por nombre 'Pretrained_Embedding'.\")\n\n        # Re-compilar con LR MUY BAJA\n        learning_rate_fase2 = 5e-5 # Probar 1e-5 o 5e-5\n        optimizer_fase2 = Adam(learning_rate=learning_rate_fase2)\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer_fase2, metrics=['accuracy'])\n        print(f\"Modelo re-compilado para Fase 2 con LR={learning_rate_fase2}.\")\n        model.summary() # Verificar cambio en parámetros entrenables\n        model_loaded_fase2 = True\n    else:\n        print(f\"ERROR: No se encontró el archivo de checkpoint de Fase 1: {checkpoint_path_fase1}\")\n\nexcept Exception as e:\n    print(f\"Error al preparar Fase 2: {e}\")","metadata":{"_uuid":"00c1aafc-18c4-4fd3-9bf0-55c6cc5eff03","_cell_guid":"b2bb7d08-db9d-456c-a9c1-ea9a4abf4881","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:51:53.654957Z","iopub.execute_input":"2025-04-29T16:51:53.655259Z","iopub.status.idle":"2025-04-29T16:51:54.008612Z","shell.execute_reply.started":"2025-04-29T16:51:53.655241Z","shell.execute_reply":"2025-04-29T16:51:54.007867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 14c. Callbacks (FASE 2)\nif model_loaded_fase2:\n    checkpoint_path_fase2 = 'best_model_emociones_emb_fase2.keras' # Nuevo nombre para Fase 2\n    model_checkpoint_fase2 = ModelCheckpoint(checkpoint_path_fase2, monitor='val_loss', save_best_only=True, verbose=1)\n    # Re-instanciar ES y ReduceLR para Fase 2 (resetea sus estados internos)\n    early_stopping_fase2 = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True) # True ahora para el final\n    reduce_lr_fase2 = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1) # Podría necesitar min_lr más bajo\n    callbacks_list_fase2 = [model_checkpoint_fase2, early_stopping_fase2, reduce_lr_fase2]\n    print(f\"Callbacks definidos para Fase 2. Checkpoint: {checkpoint_path_fase2}\")\nelse:\n    print(\"Saltando definición de callbacks de Fase 2 porque el modelo no se cargó.\")","metadata":{"_uuid":"75334c3a-3784-42c0-8501-74afab6a541c","_cell_guid":"87b95608-a395-4757-8e7b-a8a414eac095","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:51:56.443936Z","iopub.execute_input":"2025-04-29T16:51:56.444486Z","iopub.status.idle":"2025-04-29T16:51:56.449390Z","shell.execute_reply.started":"2025-04-29T16:51:56.444465Z","shell.execute_reply":"2025-04-29T16:51:56.448466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 14d. Entrenar el modelo (FASE 2 - Fine-tuning)\nhistory_fase2 = None # Inicializar\nif model_loaded_fase2:\n    epochs_fase2_extra = 30 # Cuántas épocas MÁS intentar entrenar\n    epochs_total = last_epoch_fase1 + epochs_fase2_extra\n    print(f\"\\n--- Iniciando Entrenamiento FASE 2 (Fine-tuning desde época {last_epoch_fase1 + 1}) ---\")\n    history_fase2 = model.fit(\n      X_train, y_train,\n      epochs=epochs_total,\n      initial_epoch=last_epoch_fase1 + 1, # Empezar desde la siguiente época\n      batch_size=batch_size, # Puedes probar un batch size más pequeño aquí (e.g., 64)\n      validation_data=(X_val, y_val),\n      callbacks=callbacks_list_fase2,\n      verbose=1\n    )\n    print(\"--- Entrenamiento FASE 2 Finalizado ---\")\nelse:\n    print(\"Saltando entrenamiento de Fase 2 porque el modelo no se cargó/preparó.\")","metadata":{"_uuid":"6b77950e-206c-47d6-8065-fee3472fac93","_cell_guid":"36a53655-8b06-4546-be86-e47a28c0abdb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:51:58.382541Z","iopub.execute_input":"2025-04-29T16:51:58.383149Z","iopub.status.idle":"2025-04-29T16:52:32.222276Z","shell.execute_reply.started":"2025-04-29T16:51:58.383126Z","shell.execute_reply":"2025-04-29T16:52:32.221410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 15. Visualizar el historial de entrenamiento (Fase 2 o combinado)\nprint(\"\\nGraficando historial...\")\nfinal_history = None\nif history_fase2:\n    print(\"Mostrando historial de FASE 2.\")\n    final_history = history_fase2.history\nelif history_fase1:\n     print(\"Mostrando historial de FASE 1 (Fase 2 no se ejecutó).\")\n     final_history = history_fase1.history\n\nif final_history:\n    plt.figure(figsize=(12, 5))\n    # Precisión\n    plt.subplot(1, 2, 1)\n    if 'accuracy' in final_history: plt.plot(final_history['accuracy'], label='Acc (Train)')\n    if 'val_accuracy' in final_history: plt.plot(final_history['val_accuracy'], label='Acc (Val)')\n    plt.title('Precisión (Última Fase)'); plt.xlabel('Época (relativa a la fase)'); plt.legend()\n    # Pérdida\n    plt.subplot(1, 2, 2)\n    if 'loss' in final_history: plt.plot(final_history['loss'], label='Loss (Train)')\n    if 'val_loss' in final_history: plt.plot(final_history['val_loss'], label='Loss (Val)')\n    plt.title('Pérdida (Última Fase)'); plt.xlabel('Época (relativa a la fase)'); plt.legend()\n    plt.tight_layout(); plt.show()\nelse:\n    print(\"No hay historial para graficar.\")","metadata":{"_uuid":"7edcb998-17e1-4d7f-aba9-da23ca8c15da","_cell_guid":"d57d377e-1188-4daf-bc33-6044891f56bb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:53:01.641919Z","iopub.execute_input":"2025-04-29T16:53:01.642507Z","iopub.status.idle":"2025-04-29T16:53:02.024592Z","shell.execute_reply.started":"2025-04-29T16:53:01.642485Z","shell.execute_reply":"2025-04-29T16:53:02.024026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 16. Evaluar el mejor modelo guardado (Idealmente de Fase 2)\nprint(\"\\nEvaluando el MEJOR MODELO FINAL...\")\n# El objeto 'model' debería tener los mejores pesos de Fase 2 si restore_best_weights=True\n# O podemos cargar el checkpoint de Fase 2 explícitamente para estar seguros\nbest_model_path_final = checkpoint_path_fase2 if 'checkpoint_path_fase2' in locals() and os.path.exists(checkpoint_path_fase2) else checkpoint_path_fase1\n\ntry:\n    print(f\"Cargando mejor modelo desde: {best_model_path_final}\")\n    if os.path.exists(best_model_path_final):\n        best_model = load_model(best_model_path_final)\n        loss, accuracy = best_model.evaluate(X_val, y_val, verbose=0)\n        print(f\"\\nEvaluación (Mejor Modelo): Pérdida={loss:.4f}, Precisión={accuracy:.4f}\")\n        y_pred_prob = best_model.predict(X_val); y_pred = np.argmax(y_pred_prob, axis=1)\n        y_val_labels = np.argmax(y_val, axis=1)\n        print(\"\\nInforme de Clasificación:\"); print(classification_report(y_val_labels, y_pred, target_names=label_encoder.classes_, zero_division=0))\n        print(\"\\nMatriz de Confusión:\"); cm = confusion_matrix(y_val_labels, y_pred)\n        plt.figure(figsize=(10, 8)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n        plt.xlabel('Predicha'); plt.ylabel('Verdadera'); plt.title('Matriz de Confusión'); plt.show()\n    else:\n        print(\"El archivo del mejor modelo no existe.\")\nexcept Exception as e: print(f\"\\nError evaluando: {e}\")","metadata":{"_uuid":"ed5f1a4a-27f2-4fe8-9a4f-8198cd700d7f","_cell_guid":"7a16ed91-7581-4202-9490-b44cf6df83f8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:53:04.627408Z","iopub.execute_input":"2025-04-29T16:53:04.628070Z","iopub.status.idle":"2025-04-29T16:53:08.038711Z","shell.execute_reply.started":"2025-04-29T16:53:04.628046Z","shell.execute_reply":"2025-04-29T16:53:08.037882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 17. Función de Predicción y Ejemplos\n# (Usa el 'best_model' cargado en la celda anterior)\ndef predecir_emocion(texto, modelo_pred, tokenizer_func, label_encoder_func, maxlen_func):\n  if not texto or not isinstance(texto, str): return \"Texto inválido\", 0.0\n  texto_limpio = clean_text(texto)\n  if not texto_limpio: return \"Texto vacío después de limpiar\", 0.0\n  secuencia = tokenizer_func.texts_to_sequences([texto_limpio])\n  padded_secuencia = pad_sequences(secuencia, maxlen=maxlen_func, padding='post', truncating='post')\n  prediccion_prob = modelo_pred.predict(padded_secuencia)\n  etiqueta_idx = np.argmax(prediccion_prob[0])\n  probabilidad = np.max(prediccion_prob[0])\n  etiqueta_predicha = label_encoder_func.inverse_transform([etiqueta_idx])[0]\n  return etiqueta_predicha, probabilidad\n\nif 'best_model' in locals(): # Verificar si se cargó el mejor modelo\n    predictor_model = best_model\n    print(\"\\nProbando predicción con el mejor modelo cargado:\")\n    # Ejemplos...\n    texto_prueba = \"alguna vez se deprimen, porque la única persona con la que quieren hablar y pasar el tiempo está castigada... así que solo están sentados allí, sin hacer nada, porque no tienen ganas de hablar o pasar tiempo con nadie más.\"\n    emocion_predicha, prob = predecir_emocion(texto_prueba, predictor_model, tokenizer, label_encoder, maxlen)\n    print(f\"Texto: '{texto_prueba}'\\nEmoción Predicha: {emocion_predicha} (Probabilidad: {prob:.4f})\\n\")\n    texto_prueba2 = \"me siento muy mal, no quiero vivir\"\n    emocion_predicha2, prob2 = predecir_emocion(texto_prueba2, predictor_model, tokenizer, label_encoder, maxlen)\n    print(f\"Texto: '{texto_prueba2}'\\nEmoción Predicha: {emocion_predicha2} (Probabilidad: {prob2:.4f})\\n\")\n    texto_prueba3 = \"Estoy muy feliz con los resultados del examen\"\n    emocion_predicha3, prob3 = predecir_emocion(texto_prueba3, predictor_model, tokenizer, label_encoder, maxlen)\n    print(f\"Texto: '{texto_prueba3}'\\nEmoción Predicha: {emocion_predicha3} (Probabilidad: {prob3:.4f})\\n\")\n    texto_prueba4 = \"El día estuvo normal, nada especial ocurrió, que aburrido.\"\n    emocion_predicha4, prob4 = predecir_emocion(texto_prueba4, predictor_model, tokenizer, label_encoder, maxlen)\n    print(f\"Texto: '{texto_prueba4}'\\nEmoción Predicha: {emocion_predicha4} (Probabilidad: {prob4:.4f})\\n\")\nelse:\n    print(\"\\nNo se pudo encontrar un modelo ('best_model') para realizar predicciones.\")","metadata":{"_uuid":"543a9a7f-2704-4986-92c6-8a8c6ed94e94","_cell_guid":"09781e59-e016-4612-bf93-dd1e42dea4ae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T16:53:11.715487Z","iopub.execute_input":"2025-04-29T16:53:11.716174Z","iopub.status.idle":"2025-04-29T16:53:12.167206Z","shell.execute_reply.started":"2025-04-29T16:53:11.716148Z","shell.execute_reply":"2025-04-29T16:53:12.166543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 18. Guardar artefactos finales\n# (Código igual, guarda tokenizer y encoder)\ntry:\n  with open('tokenizer_emociones.pickle', 'wb') as h: pickle.dump(tokenizer, h, protocol=pickle.HIGHEST_PROTOCOL)\n  print(\"Tokenizer guardado.\")\nexcept Exception as e: print(f\"Error guardando tokenizer: {e}\")\ntry:\n  with open('label_encoder_emociones.pickle', 'wb') as h: pickle.dump(label_encoder, h, protocol=pickle.HIGHEST_PROTOCOL)\n  print(\"Label Encoder guardado.\")\nexcept Exception as e: print(f\"Error guardando label encoder: {e}\")\n\nprint(f\"\\nMejor modelo Fase 1 guardado como: {checkpoint_path_fase1}\")\nif 'checkpoint_path_fase2' in locals(): print(f\"Mejor modelo Fase 2 guardado como: {checkpoint_path_fase2}\")","metadata":{"_uuid":"9effe8f4-9096-4d7b-ae1b-f601381bbba6","_cell_guid":"7177a132-86c8-4bf9-8f18-19f9bb474570","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-29T17:47:13.286773Z","iopub.execute_input":"2025-04-29T17:47:13.287036Z","iopub.status.idle":"2025-04-29T17:47:13.352957Z","shell.execute_reply.started":"2025-04-29T17:47:13.287015Z","shell.execute_reply":"2025-04-29T17:47:13.352129Z"}},"outputs":[{"name":"stdout","text":"Error guardando tokenizer: name 'pickle' is not defined\nError guardando label encoder: name 'pickle' is not defined\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3240875835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error guardando label encoder: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nMejor modelo Fase 1 guardado como: {checkpoint_path_fase1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'checkpoint_path_fase2'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mejor modelo Fase 2 guardado como: {checkpoint_path_fase2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'checkpoint_path_fase1' is not defined"],"ename":"NameError","evalue":"name 'checkpoint_path_fase1' is not defined","output_type":"error"}],"execution_count":1}]}